{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "958ac8d23ed132ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import geopandas as gpd\n",
    "import movingpandas as mpd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from shapely.geometry import Point\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from srai.datasets import PortoTaxiDataset\n",
    "from srai.embedders import Hex2VecEmbedder\n",
    "from srai.joiners import IntersectionJoiner\n",
    "from srai.loaders import OSMPbfLoader\n",
    "from srai.loaders.osm_loaders.filters import HEX2VEC_FILTER\n",
    "from srai.neighbourhoods import H3Neighbourhood\n",
    "from srai.regionalizers import H3Regionalizer\n",
    "from srai.regionalizers import geocode_to_region_gdf\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ],
   "id": "ef7953267413ca07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gpd.options.io_engine = \"pyogrio\"",
   "id": "961bd77b0b4f2801",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Enable loading Environment Variables",
   "id": "d52a544e7dce7b24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext dotenv\n",
    "\n",
    "%dotenv"
   ],
   "id": "20d6ecac9c5bc0ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "hf_token = os.getenv(\"HF_TOKEN\")",
   "id": "8403af268dcd02e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Loading",
   "id": "29162307d05be5c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "subset_size = 50_000\n",
    "use_subset = True\n",
    "\n",
    "gdf_porto_taxi_full_path = os.path.join(\"data\", \"porto_taxi.feather\")\n",
    "gdf_porto_taxi_subset_path = os.path.join(\n",
    "    \"data\", f\"porto_taxi_subset_{subset_size}.feather\"\n",
    ")"
   ],
   "id": "1d1b7d27259b9d03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not use_subset:\n",
    "    if not os.path.exists(gdf_porto_taxi_full_path):\n",
    "        porto_taxi_dataset = PortoTaxiDataset()\n",
    "        gdf_porto_taxi = porto_taxi_dataset.load(hf_token=hf_token)\n",
    "        gdf_porto_taxi.to_feather(gdf_porto_taxi_full_path)\n",
    "    else:\n",
    "        gdf_porto_taxi = gpd.read_feather(gdf_porto_taxi_full_path)\n",
    "else:\n",
    "    if not os.path.exists(gdf_porto_taxi_subset_path):\n",
    "        if not os.path.exists(gdf_porto_taxi_full_path):\n",
    "            porto_taxi_dataset = PortoTaxiDataset()\n",
    "            gdf_porto_taxi = porto_taxi_dataset.load(hf_token=hf_token)\n",
    "            gdf_porto_taxi.to_feather(gdf_porto_taxi_full_path)\n",
    "            gdf_porto_taxi = gdf_porto_taxi.head(subset_size)\n",
    "            gdf_porto_taxi.to_feather(gdf_porto_taxi_subset_path)\n",
    "        else:\n",
    "            gdf_porto_taxi = gpd.read_feather(gdf_porto_taxi_full_path)\n",
    "            gdf_porto_taxi = gdf_porto_taxi.head(subset_size)\n",
    "            gdf_porto_taxi.to_feather(gdf_porto_taxi_subset_path)\n",
    "    else:\n",
    "        gdf_porto_taxi = gpd.read_feather(gdf_porto_taxi_subset_path)"
   ],
   "id": "7c27fd03333ea15e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gdf_porto_taxi.drop(\n",
    "    [\n",
    "        \"taxi_id\",\n",
    "        \"call_type\",\n",
    "        \"origin_call\",\n",
    "        \"origin_stand\",\n",
    "        \"day_type\",\n",
    "        \"travel_time_seconds\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")"
   ],
   "id": "a6919959b8c2cd06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Convert LineString to Point",
   "id": "380443dcb3dcd7b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "exploded_rows = []\n",
    "\n",
    "for idx, row in tqdm(gdf_porto_taxi.iterrows(), total=gdf_porto_taxi.shape[0]):\n",
    "    start_timestamp = row.timestamp\n",
    "    current_timestamp = start_timestamp\n",
    "    for xy in row.geometry.coords:\n",
    "        point = Point(xy)\n",
    "        row_dict = row.to_dict()\n",
    "        row_dict[\"geometry\"] = point\n",
    "        row_dict[\"timestamp\"] = current_timestamp\n",
    "        current_timestamp += 15\n",
    "        exploded_rows.append(row_dict)"
   ],
   "id": "936f31b3a0cc8aa3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gdf_porto_taxi_points = gpd.GeoDataFrame(exploded_rows, crs=\"EPSG:4326\")",
   "id": "48d065f2708ae94a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gdf_porto_taxi_points[\"timestamp\"] = gdf_porto_taxi_points[\"timestamp\"].apply(\n",
    "    lambda x: datetime.fromtimestamp(x)\n",
    ")"
   ],
   "id": "3dbeb52b0a1d657f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "bcf8e60ad55307da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Restricting to Porto Area",
   "id": "302715a7b4dd710b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "porto_area = geocode_to_region_gdf(\"Porto District, Portugal\")",
   "id": "de8406c1333906c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gdf_porto_taxi_points_inside_porto = gdf_porto_taxi_points.sjoin(porto_area)",
   "id": "505b083a9864de45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gdf_merged = gdf_porto_taxi_points.merge(\n",
    "    gdf_porto_taxi_points_inside_porto, how=\"left\", indicator=True\n",
    ")\n",
    "df_porto_taxi_points_outside_porto = gdf_merged[gdf_merged[\"_merge\"] == \"left_only\"]"
   ],
   "id": "ab2d73c36fc61319",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trajectories_outside_porto = list(\n",
    "    df_porto_taxi_points_outside_porto[\"trip_id\"].unique()\n",
    ")"
   ],
   "id": "e594f4706dfb3603",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gdf_porto_taxi_points = gdf_porto_taxi_points[\n",
    "    ~gdf_porto_taxi_points[\"trip_id\"].isin(trajectories_outside_porto)\n",
    "]"
   ],
   "id": "2699ae75caa910c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Trajectory Collection",
   "id": "dd6414af7a53d8f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trajectory_collection = mpd.TrajectoryCollection(\n",
    "    data=gdf_porto_taxi_points, traj_id_col=\"trip_id\", t=\"timestamp\"\n",
    ")"
   ],
   "id": "58bad264056edd84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Speed calculation",
   "id": "52b04718ac5260ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trajectory_collection.add_speed(units=(\"km\", \"h\"), n_threads=24, overwrite=True)",
   "id": "6cfbe8147cffa140",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Outliers removal",
   "id": "1d6a4ab24cd3113e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trajectory_collection = mpd.OutlierCleaner(trajectory_collection).clean(\n",
    "    v_max=120, units=(\"km\", \"h\")\n",
    ")"
   ],
   "id": "d41741f2d3ef6669",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "filtered_trajectory_collection = [\n",
    "    trajectory\n",
    "    for trajectory in trajectory_collection.trajectories\n",
    "    if trajectory.size() >= 10\n",
    "]"
   ],
   "id": "18df5f8031b30819",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trajectory_collection = mpd.TrajectoryCollection(filtered_trajectory_collection)",
   "id": "f64387a8e700fc0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generalization",
   "id": "7282822ee2ef989e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trajectory_collection = mpd.DouglasPeuckerGeneralizer(trajectory_collection).generalize(\n",
    "    tolerance=0.0001\n",
    ")"
   ],
   "id": "4ead85b0732636fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conversion to Point GeoDataFrame",
   "id": "382b4c79e753ceb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gdf_trajectory_point_collection = trajectory_collection.to_point_gdf().sort_values(\n",
    "    by=[\"trip_id\", \"timestamp\"]\n",
    ")"
   ],
   "id": "369e97bb1a3b193c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# gdf_trajectory_point_collection[[\"speed\", \"distance\"]] = scaler.fit_transform(\n",
    "#     gdf_trajectory_point_collection[[\"speed\", \"distance\"]]\n",
    "# )"
   ],
   "id": "e424b9ad75705c02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Add Spatial Embedding",
   "id": "1ba24cd6e45bf458"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "regionalizer = H3Regionalizer(resolution=9)\n",
    "gdf_regions = regionalizer.transform(gdf_trajectory_point_collection)"
   ],
   "id": "77cf5d3553c9da75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loader = OSMPbfLoader()\n",
    "gdf_features = loader.load(gdf_regions, HEX2VEC_FILTER)"
   ],
   "id": "460fbf206cda2f0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "joiner = IntersectionJoiner()\n",
    "gdf_joint = joiner.transform(gdf_regions, gdf_features)"
   ],
   "id": "a981162c3e918064",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "neighbourhood = H3Neighbourhood(gdf_regions)",
   "id": "ca328f2b8fed3b07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embedder_hidden_sizes = [150, 100, 50, 10]\n",
    "embedder = Hex2VecEmbedder(embedder_hidden_sizes)\n",
    "\n",
    "df_embeddings = embedder.fit_transform(\n",
    "    gdf_regions,\n",
    "    gdf_features,\n",
    "    gdf_joint,\n",
    "    neighbourhood,\n",
    "    trainer_kwargs={\"max_epochs\": 15, \"accelerator\": device},\n",
    "    batch_size=64,\n",
    ")"
   ],
   "id": "3da45a83f647f521",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_embeddings.rename(\n",
    "    columns={\n",
    "        0: \"embedding_0\",\n",
    "        1: \"embedding_1\",\n",
    "        2: \"embedding_2\",\n",
    "        3: \"embedding_3\",\n",
    "        4: \"embedding_4\",\n",
    "        5: \"embedding_5\",\n",
    "        6: \"embedding_6\",\n",
    "        7: \"embedding_7\",\n",
    "        8: \"embedding_8\",\n",
    "        9: \"embedding_9\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ],
   "id": "2f9c69c996bf27dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gdf_joined = gpd.sjoin(gdf_trajectory_point_collection, gdf_regions, how=\"left\")",
   "id": "3e026b175f675aa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gdf_joined.rename(ckolumns={\"index_right\": \"region_id\"}, inplace=True)\n",
    "gdf_joined.reset_index(inplace=True)"
   ],
   "id": "25550f41532e45a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gdf_points_embeddings = gdf_joined.merge(df_embeddings, on=\"region_id\", how=\"left\")",
   "id": "6a30a6295fa799d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preparing DataSet",
   "id": "53c34e4dc157c887"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, df, seq_length):\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        self.seq_length = seq_length\n",
    "        self._create_sequences(df)\n",
    "\n",
    "    def _create_sequences(self, df):\n",
    "        for trajectory_id in df[\"trip_id\"].unique():\n",
    "            trajectory_data = df[df[\"trip_id\"] == trajectory_id]\n",
    "            for i in range(len(trajectory_data) - self.seq_length):\n",
    "                sequence = trajectory_data.iloc[i : i + self.seq_length]\n",
    "                travel_time = (\n",
    "                    sequence[\"timestamp\"].iloc[-1] - sequence[\"timestamp\"].iloc[0]\n",
    "                ).total_seconds()\n",
    "                self.sequences.append(\n",
    "                    sequence[\n",
    "                        [\n",
    "                            \"embedding_0\",\n",
    "                            \"embedding_1\",\n",
    "                            \"embedding_2\",\n",
    "                            \"embedding_3\",\n",
    "                            \"embedding_4\",\n",
    "                            \"embedding_5\",\n",
    "                            \"embedding_6\",\n",
    "                            \"embedding_7\",\n",
    "                            \"embedding_8\",\n",
    "                            \"embedding_9\",\n",
    "                        ]\n",
    "                    ].values\n",
    "                )\n",
    "                self.labels.append(travel_time)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(\n",
    "            self.labels[idx], dtype=torch.float32\n",
    "        )"
   ],
   "id": "b78d6a3341198f49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seq_length = 10\n",
    "dataset = TrajectoryDataset(gdf_points_embeddings, seq_length)"
   ],
   "id": "672ea4b8e4b70ba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, test_size]\n",
    ")"
   ],
   "id": "4877d81bc7e88fe6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=24, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, num_workers=24, shuffle=False)"
   ],
   "id": "d80c4cb3725832c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TravelTimeLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(TravelTimeLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "input_size = 10\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "\n",
    "model = TravelTimeLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "model = model.to(device)"
   ],
   "id": "b3b79d58e3cdf0bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 10"
   ],
   "id": "21e5a8dd06317e63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for sequences, labels in train_loader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ],
   "id": "63aa22adc932a338",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in test_loader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ],
   "id": "cf7a300c3272e909",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in test_loader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        outputs = model(sequences)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        actuals.extend(labels.cpu().numpy())"
   ],
   "id": "dda0b9681b06f14f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(actuals, color=\"blue\", label=\"Actual Travel Time\")\n",
    "plt.plot(predictions, color=\"red\", label=\"Predicted Travel Time\")\n",
    "plt.title(\"Travel Time Prediction\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Travel Time\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "22815278f616c34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "mean_time_travelled = np.mean(actuals)\n",
    "print(f\"Mean average travelled time: {mean_time_travelled}\")\n",
    "\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")"
   ],
   "id": "3fcf3233ab4a7e97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "np.min(actuals), np.max(actuals), np.mean(actuals), np.median(actuals)",
   "id": "2f4bd60bb6c31435",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "np.min(predictions), np.max(predictions), np.mean(predictions), np.median(predictions)",
   "id": "6ef7db552f3e44",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
