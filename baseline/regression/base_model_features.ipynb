{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from srai.datasets import AirbnbMulticityDataset\n",
    "from srai.embedders import Hex2VecEmbedder\n",
    "from srai.joiners import IntersectionJoiner\n",
    "from srai.loaders.osm_loaders import OSMPbfLoader\n",
    "from srai.loaders.osm_loaders.filters import HEX2VEC_FILTER\n",
    "from srai.neighbourhoods.h3_neighbourhood import H3Neighbourhood\n",
    "from srai.plotting import plot_regions\n",
    "from srai.regionalizers import H3Regionalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 9\n",
    "embedder_hidden_sizes = [150, 100, 50]\n",
    "max_epochs_embedder = 10\n",
    "batch_size_embedder = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "airbnb_multicity = AirbnbMulticityDataset()\n",
    "airbnb_multicity_gdf = airbnb_multicity.load(hf_token=hf_token)\n",
    "gdf_paris = airbnb_multicity_gdf.loc[\n",
    "    airbnb_multicity_gdf[\"city\"].isin([\"paris\", \"rotterdam\"])\n",
    "]  # , \"rotterdam\", \"brussels\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_multicity_gdf.city.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regionalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionalizer = H3Regionalizer(resolution=resolution)\n",
    "regions = regionalizer.transform(gdf_paris)\n",
    "plot_regions(regions_gdf=regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OSM loader & joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = OSMPbfLoader()\n",
    "features = loader.load(gdf_paris, HEX2VEC_FILTER)\n",
    "joiner = IntersectionJoiner()\n",
    "joint = joiner.transform(regions, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3 Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood = H3Neighbourhood(regions)\n",
    "embedder = Hex2VecEmbedder(embedder_hidden_sizes)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    embeddings = embedder.fit_transform(\n",
    "        regions,\n",
    "        features,\n",
    "        joint,\n",
    "        neighbourhood,\n",
    "        trainer_kwargs={\"max_epochs\": max_epochs_embedder, \"accelerator\": device},\n",
    "        batch_size=batch_size_embedder,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_size = embeddings.values.shape[1]\n",
    "print(f\"Embeddings size: {embeddings_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_columns(row) -> np.ndarray:\n",
    "    \"\"\"Concatenate embedding values together.\n",
    "\n",
    "    Args:\n",
    "        row (_type_): row of embeddings\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: concatenated embedding\n",
    "    \"\"\"\n",
    "    return np.concatenate([np.atleast_1d(val) for val in row.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf = gpd.sjoin(gdf_paris, regions, how=\"left\", op=\"within\")\n",
    "joined_gdf.rename(columns={\"index_right\": \"h3_index\"}, inplace=True)\n",
    "# getting avg price per h3\n",
    "average_hex_prices = joined_gdf.groupby(\"h3_index\")[\"price\"].mean()\n",
    "average_hex_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_paris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"price\"]\n",
    "features_to_add = [\n",
    "    \"number_of_reviews\",\n",
    "    \"minimum_nights\",\n",
    "    \"availability_365\",\n",
    "    \"calculated_host_listings_count\",\n",
    "    \"number_of_reviews_ltm\",\n",
    "]\n",
    "input_features = [\"vector_embedding\"] + features_to_add\n",
    "columns_to_add = features_to_add + target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting avg price per h3\n",
    "averages_hex = joined_gdf.groupby(\"h3_index\")[columns_to_add].mean()\n",
    "averages_hex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(gdf_paris[columns_to_add])\n",
    "# gdf_paris[columns_to_add].head()\n",
    "# standarized = pd.DataFrame(scaler.transform(gdf_paris[columns_to_add]), columns=columns_to_add)\n",
    "# standarized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[\"h3\"] = embeddings.index\n",
    "\n",
    "merged_gdf = embeddings.merge(averages_hex, how=\"inner\", left_on=\"region_id\", right_on=\"h3_index\")\n",
    "merged_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_columns = [col for col in merged_gdf.columns if col not in ([\"h3\"] + target)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_gdf[merge_columns].apply(concat_columns, axis=1).values\n",
    "X_h3_idx = merged_gdf[\"h3\"].values\n",
    "y = merged_gdf[\"price\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True)\n",
    "X_train = torch.tensor(X_train.tolist(), dtype=torch.float32).cuda()\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).cuda()\n",
    "X_test = torch.tensor(X_test.tolist(), dtype=torch.float32).cuda()\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    n_epochs,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    batch_size,\n",
    "    batch_start,\n",
    ") -> tuple[nn.Module, list, list]:\n",
    "    best_mse = np.inf  # init to infinity\n",
    "    best_weights = None\n",
    "    l1_loss_eval = []\n",
    "    l1_loss_train = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_list = []\n",
    "        model.train()\n",
    "        with tqdm(batch_start, unit=\"batch\", mininterval=0) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start : start + batch_size]\n",
    "                y_batch = y_train[start : start + batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                # bar.set_postfix(mse=float(loss))\n",
    "                # mse_train.append(loss.item())\n",
    "                loss_list.append(loss.item())\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}], avg_loss: {np.mean(loss_list):.4f}\")\n",
    "        l1_loss_train.append(np.mean(loss_list))\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            y_pred = model(X_test)\n",
    "            # y_pred_train= model(X_train)\n",
    "            # mae_train=loss_fn(y_pred_train, y_train)\n",
    "            mse = loss_fn(y_pred, y_test)\n",
    "            # mse_train.append(float(mae_train))\n",
    "            l1_loss_eval.append(float(mse))\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return model, l1_loss_train, l1_loss_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionBaseModel(nn.Module):\n",
    "    \"\"\"Regression base module.\n",
    "\n",
    "    Definition of Regression Module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embeddings_size, linear_sizes=None):\n",
    "        \"\"\"Initializaiton of regression module.\n",
    "\n",
    "        Args:\n",
    "            embeddings_size (_type_): size of input embedding\n",
    "            linear_sizes (_type_, optional): sizes of linear layers inside module. \\\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if linear_sizes is None:\n",
    "            linear_sizes = [500, 1000]\n",
    "        self.model = torch.nn.Sequential()\n",
    "        previous_size = embeddings_size\n",
    "        for cnt, size in enumerate(linear_sizes):\n",
    "            self.model.add_module(f\"linear_{cnt}\", nn.Linear(previous_size, size))\n",
    "            self.model.add_module(f\"ReLU_{cnt}\", nn.ReLU())\n",
    "            previous_size = size\n",
    "            if cnt % 2:\n",
    "                self.model.add_module(f\"dropout_{cnt}\", nn.Dropout(p=0.2))\n",
    "        self.model.add_module(\"linear_final\", nn.Linear(previous_size, 1))\n",
    "\n",
    "    def forward(self, x):  # noqa: D102\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 30\n",
    "lr = 0.001\n",
    "linear_sizes = [500, 1000]\n",
    "\n",
    "batch_start = torch.arange(0, len(X_train), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionBaseModel(embeddings_size=X_train.shape[1], linear_sizes=linear_sizes)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_model, loss_train, loss_eval = train(\n",
    "    model, n_epochs, optimizer, loss_fn, batch_size, batch_start\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 9))\n",
    "\n",
    "ax[0].plot(loss_eval)\n",
    "ax[1].plot(loss_train)\n",
    "\n",
    "ax[0].set_xlabel(\"epoch\")\n",
    "ax[0].set_ylabel(\"L1 loss eval value\")\n",
    "ax[0].set_title(\"Plot of L1 loss eval results\")\n",
    "\n",
    "ax[1].set_xlabel(\"epoch\")\n",
    "ax[1].set_ylabel(\"L1 loss train value\")\n",
    "ax[1].set_title(\"Plot of L1 loss train results\")\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(loss_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
