{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from datasets import load_dataset\n",
    "import h3\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from srai.embedders import Hex2VecEmbedder, CountEmbedder\n",
    "from srai.joiners import IntersectionJoiner\n",
    "from srai.regionalizers import H3Regionalizer, geocode_to_region_gdf\n",
    "from srai.loaders.osm_loaders.filters import HEX2VEC_FILTER\n",
    "from srai.loaders.osm_loaders import OSMPbfLoader\n",
    "from srai.neighbourhoods import H3Neighbourhood\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building representation vectors for a given city with OSM features (filtered to the ones used in hex2vec embedder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY = \"Paris\"\n",
    "COUNTRY = \"FRANCE\"\n",
    "HEX_RESOLUTION = 10\n",
    "HF_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = f\"{CITY}, {COUNTRY}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting osm features from given area. !TAKES TIME + doesnt work on windows? !\n",
    "loader = OSMPbfLoader()\n",
    "area_gdf = geocode_to_region_gdf(location)\n",
    "features_gdf = loader.load(area_gdf, HEX2VEC_FILTER)\n",
    "features_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all h3 indexes in given resolution together with their polygon geometry (important for later!)\n",
    "regionalizer = H3Regionalizer(resolution=HEX_RESOLUTION)\n",
    "regions_gdf = regionalizer.transform(area_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint df of hexes and their features\n",
    "joiner = IntersectionJoiner()\n",
    "joint_gdf = joiner.transform(regions_gdf, features_gdf)\n",
    "joint_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For He2vecEmbedder neccessery to correct error in library\n",
    "# In srai library, one has to cast neighbourhoods to set in neighbourhoods/_base.py\n",
    "\n",
    "# embedding_layerss=[200,100,50]\n",
    "# embedding_size=embeddings_layers[-1]\n",
    "# neighbourhood = H3Neighbourhood(regions_gdf)\n",
    "# embedder = Hex2VecEmbedder(embeddings_layers)\n",
    "\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter(\"ignore\")\n",
    "#     embeddings = embedder.fit_transform(\n",
    "#         regions_gdf,\n",
    "#         features_gdf,\n",
    "#         joint_gdf,\n",
    "#         neighbourhood,\n",
    "#         trainer_kwargs={\"max_epochs\": 5, \"accelerator\": \"gpu\"},\n",
    "#         batch_size=100,\n",
    "#     )\n",
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count embedder\n",
    "embedder = CountEmbedder()\n",
    "embeddings = embedder.transform(regions_gdf, features_gdf, joint_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get representation vector from counts\n",
    "def concat_columns(row):\n",
    "    return np.concatenate([np.atleast_1d(val) for val in row.values])\n",
    "\n",
    "\n",
    "embedding_size = len(embeddings.columns)\n",
    "embeddings[\"vector_embedding\"] = embeddings.apply(concat_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping of airbnb data to h3 indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"kraina/airbnb_multicity\", use_auth_token=HF_KEY)\n",
    "df = gpd.GeoDataFrame(dataset[\"train\"].to_pandas())\n",
    "data_gdf = gpd.GeoDataFrame(\n",
    "    df.drop([\"latitude\", \"longitude\"], axis=1),\n",
    "    geometry=gpd.points_from_xy(x=df[\"longitude\"], y=df[\"latitude\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we got embeddings for particular city\n",
    "data_gdf = data_gdf.loc[data_gdf[\"city\"] == CITY.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_regions_gdf = regionalizer.transform(data_gdf)\n",
    "# assigns points into a h3 index ( 'within' polygon )\n",
    "data_joined_gdf = gpd.sjoin(data_gdf, data_regions_gdf, how=\"left\", op=\"within\")\n",
    "data_joined_gdf.rename(columns={\"index_right\": \"h3_index\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with index as H3 names and column as average prices within\n",
    "average_prices = data_joined_gdf.groupby(\"h3_index\")[\"price\"].mean()\n",
    "average_prices_df = pd.DataFrame({\"average_price\": average_prices})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining data with embeddings vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workaround because I kept loosing index somehow after merging those\n",
    "embeddings[\"h3\"] = embeddings.index\n",
    "merged_gdf = embeddings.merge(\n",
    "    average_prices_df, how=\"inner\", left_on=\"region_id\", right_on=\"h3_index\"\n",
    ")\n",
    "# We need to think how it should work -> for now, inner join results just in regions that are both\n",
    "# in area hexes and airbnb daya, so it leaves out regions that were not included in data - question, should we include them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prep for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_gdf[\"vector_embedding\"].values\n",
    "X_h3_idx = merged_gdf[\"h3\"].values\n",
    "y = merged_gdf[\"average_price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# While splitting to train test and dev, we should keep which indexes belong to which split\n",
    "# so then we could divide X_h3_idx to same splits as well. This ways we could map\n",
    "# h3_index to its vector and use it for single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "X_train = torch.tensor(X_train.tolist(), dtype=torch.float32).cuda()\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).cuda()\n",
    "X_test = torch.tensor(X_test.tolist(), dtype=torch.float32).cuda()\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple model & training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding size comes from embedders -> in count embedder its number of features,\n",
    "#  in hex2vec you define latent space dim\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(embedding_size, 225),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(225, 100),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(100, 50),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(50, 25),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(25, 1),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "model.cuda()\n",
    "loss_fn = nn.MSELoss()  # mean square error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "best_mse = np.inf  # init to infinity\n",
    "best_weights = None\n",
    "mse_eval = []\n",
    "mse_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with tqdm(batch_start, unit=\"batch\", mininterval=0) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for start in bar:\n",
    "            # take a batch\n",
    "            X_batch = X_train[start : start + batch_size]\n",
    "            y_batch = y_train[start : start + batch_size]\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print progress\n",
    "            bar.set_postfix(mse=float(loss))\n",
    "            mse_train.append(float(loss))\n",
    "    # evaluate at end of each epoch\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    mse = loss_fn(y_pred, y_test)\n",
    "    mse = float(mse)\n",
    "    mse_eval.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# restore model and return best accuracy\n",
    "model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse_eval)\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE eval value\")\n",
    "plt.title(\"Plot of MSE results\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
